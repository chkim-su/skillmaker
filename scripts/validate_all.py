#!/usr/bin/env python3
"""
Unified plugin validation and auto-fix script.

Runs all automated checks and optionally fixes issues:
- Registration integrity (marketplace.json ‚Üî files)
- Frontmatter validation (YAML syntax, required fields)
- Directory structure validation
- Settings.json validation

Usage:
    python3 scripts/validate_all.py [plugin-path]
    python3 scripts/validate_all.py --fix           # Auto-fix issues
    python3 scripts/validate_all.py --fix --dry-run # Preview fixes
    python3 scripts/validate_all.py --json          # JSON output

Exit codes:
    0 - All passed (or all fixed with --fix)
    1 - Errors found (deployment will fail)
    2 - Warnings only (deployment may work)
"""

import json
import sys
import os
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional

try:
    import yaml
except ImportError:
    # Fallback: simple YAML parser for frontmatter
    yaml = None


class Fix:
    """Represents a fixable issue."""
    def __init__(self, description: str, fix_func, *args):
        self.description = description
        self.fix_func = fix_func
        self.args = args

    def apply(self) -> bool:
        """Apply the fix. Returns True if successful."""
        try:
            self.fix_func(*self.args)
            return True
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Fix failed: {e}")
            return False


class ValidationResult:
    def __init__(self):
        self.errors: List[str] = []
        self.warnings: List[str] = []
        self.passed: List[str] = []
        self.fixes: List[Fix] = []  # Auto-fixable issues

    def add_error(self, msg: str, fix: Optional[Fix] = None):
        self.errors.append(msg)
        if fix:
            self.fixes.append(fix)

    def add_warning(self, msg: str, fix: Optional[Fix] = None):
        self.warnings.append(msg)
        if fix:
            self.fixes.append(fix)

    def add_pass(self, msg: str):
        self.passed.append(msg)

    def merge(self, other: 'ValidationResult'):
        self.errors.extend(other.errors)
        self.warnings.extend(other.warnings)
        self.passed.extend(other.passed)
        self.fixes.extend(other.fixes)


# =============================================================================
# SKILL REFERENCES: Map warnings/problems to skillmaker skills with solutions
# =============================================================================
# When validation detects issues, include reference to which skill has the solution
# Format: { "warning_pattern": ("skill_name", "reference_file", "brief_solution") }

SKILL_REFERENCES = {
    # W028: MUST/CRITICAL without hooks
    "W028": (
        "hook-templates",
        "references/full-examples.md",
        "PreToolUse/PostToolUseÎ°ú ÌñâÎèô Í∞ïÏ†ú"
    ),
    # W029: Skill frontmatter missing
    "W029": (
        "skill-design",
        "references/structure-rules.md",
        "YAML frontmatter: name, description, allowed-tools"
    ),
    # W030: Agent frontmatter missing tools
    "W030": (
        "orchestration-patterns",
        "references/context-isolation.md",
        "tools: [] = no MCP access; tools: omitted = all tools"
    ),
    # W031/W032: Skill content too long / missing references
    "W031": (
        "skill-design",
        "references/progressive-disclosure.md",
        "ÌïµÏã¨ <500words, ÏÉÅÏÑ∏Îäî references/Î°ú Î∂ÑÎ¶¨"
    ),
    "W032": (
        "skill-design",
        "references/progressive-disclosure.md",
        "references/ ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ± ÌõÑ ÏÉÅÏÑ∏ ÎÇ¥Ïö© Ïù¥Îèô"
    ),
    # W033: Missing Skill() usage
    "W033": (
        "orchestration-patterns",
        "references/skill-loading-patterns.md",
        "Skill() ÎèÑÍµ¨Î°ú Î™ÖÏãúÏ†Å Ïä§ÌÇ¨ Î°úÎî©"
    ),
    # W034: Multi-stage workflow without per-stage skill loading
    "W034": (
        "workflow-state-patterns",
        "references/complete-workflow-example.md",
        "Îã®Í≥ÑÎ≥Ñ Skill() Î°úÎî© ÎòêÎäî hook-based ÏûêÎèô Î°úÎî©"
    ),
    # W035: NOT YET HOOKIFIED markers
    "W035": (
        "hook-templates",
        "references/full-examples.md",
        "PreToolUse hookÏúºÎ°ú Í∞ïÏ†ú Íµ¨ÌòÑ"
    ),
    # MCP/Gateway related (detected by keywords)
    "MCP_GATEWAY": (
        "mcp-gateway-patterns",
        "references/daemon-shared-server.md",
        "Daemon (SSE) Ìå®ÌÑ¥: python -m server --sse --port 8080"
    ),
    # Agent tools:[] issue (MCP access)
    "AGENT_NO_MCP": (
        "mcp-gateway-patterns",
        "references/agent-gateway-template.md",
        "tools: [] = MCP Ï†ëÍ∑º Î∂àÍ∞Ä. Daemon Ìå®ÌÑ¥ ÎòêÎäî tools Î™ÖÏãú ÌïÑÏöî"
    ),
}


def get_skill_hint(warning_code: str, context: str = "") -> str:
    """Get skill reference hint for a warning code."""
    ref = SKILL_REFERENCES.get(warning_code)
    if not ref:
        # Check for context-based hints
        if "gateway" in context.lower() or "mcp" in context.lower() or "subagent" in context.lower():
            ref = SKILL_REFERENCES.get("MCP_GATEWAY")
        elif "tools" in context.lower() and "[]" in context:
            ref = SKILL_REFERENCES.get("AGENT_NO_MCP")

    if ref:
        skill_name, ref_file, brief = ref
        return f"\n       ‚Üí Ìï¥Í≤∞: skillmaker:{skill_name} | {ref_file} | {brief}"
    return ""


def find_marketplace_json(start_path: Path) -> Tuple[Path | None, str | None]:
    """
    Find marketplace.json in .claude-plugin/ directory.

    Returns:
        (path, warning) - path to marketplace.json and optional warning message
    """
    claude_plugin = start_path / ".claude-plugin"
    if claude_plugin.exists():
        marketplace = claude_plugin / "marketplace.json"
        if marketplace.exists():
            return marketplace, None

    # Check for legacy plugin.json (not supported by Claude Code)
    plugin_json = start_path / "plugin.json"
    if plugin_json.exists():
        warning = (
            "‚ö†Ô∏è LEGACY FORMAT: Found plugin.json but Claude Code requires .claude-plugin/marketplace.json\n"
            "   plugin.json is NOT recognized during installation.\n"
            "   ‚Üí Migration required: Move plugin.json to .claude-plugin/marketplace.json"
        )
        return plugin_json, warning

    return None, None


def parse_frontmatter(content: str) -> Tuple[dict | None, str | None]:
    """Parse YAML frontmatter from markdown content."""
    if not content.startswith('---'):
        return None, "No frontmatter found"

    try:
        end_idx = content.index('---', 3)
        yaml_content = content[3:end_idx].strip()
        if yaml:
            return yaml.safe_load(yaml_content), None
        else:
            # Simple fallback parser
            result = {}
            for line in yaml_content.split('\n'):
                if ':' in line:
                    key, value = line.split(':', 1)
                    result[key.strip()] = value.strip().strip('"').strip("'")
            return result, None
    except (ValueError, Exception) as e:
        return None, str(e)


# ============================================================================
# FIX FUNCTIONS
# ============================================================================

def fix_add_to_marketplace(marketplace_path: Path, item_type: str, item_path: str):
    """Add an item to marketplace.json."""
    data = json.loads(marketplace_path.read_text())
    plugins = data.get("plugins", [data])

    for plugin in plugins:
        if item_type not in plugin:
            plugin[item_type] = []

        # Normalize path format
        if not item_path.startswith("./"):
            item_path = f"./{item_path}"

        if item_path not in plugin[item_type]:
            plugin[item_type].append(item_path)

    # Write back with proper formatting
    marketplace_path.write_text(json.dumps(data, indent=2) + "\n")


def fix_remove_from_marketplace(marketplace_path: Path, item_type: str, item_path: str):
    """Remove an item from marketplace.json."""
    data = json.loads(marketplace_path.read_text())
    plugins = data.get("plugins", [data])

    for plugin in plugins:
        items = plugin.get(item_type, [])
        # Try both formats
        for fmt in [item_path, f"./{item_path}", item_path.lstrip("./")]:
            if fmt in items:
                items.remove(fmt)
                break
        plugin[item_type] = items

    marketplace_path.write_text(json.dumps(data, indent=2) + "\n")


def fix_create_command_stub(cmd_path: Path, name: str):
    """Create a stub command file with proper frontmatter."""
    content = f'''---
description: TODO: Add description for {name}
argument-hint: "[optional args]"
allowed-tools: ["Read", "Write", "Bash", "Grep", "Glob"]
---

# {name.replace('-', ' ').title()}

TODO: Add command instructions here.
'''
    cmd_path.parent.mkdir(parents=True, exist_ok=True)
    cmd_path.write_text(content)


def fix_create_agent_stub(agent_path: Path, name: str):
    """Create a stub agent file with proper frontmatter."""
    content = f'''---
name: {name}
description: TODO: Add description for {name}
tools: ["Read", "Write", "Bash", "Grep", "Glob"]
model: sonnet
---

# {name.replace('-', ' ').title()} Agent

TODO: Add agent instructions here.
'''
    agent_path.parent.mkdir(parents=True, exist_ok=True)
    agent_path.write_text(content)


def fix_create_skill_stub(skill_dir: Path, name: str):
    """Create a stub SKILL.md file."""
    skill_md = skill_dir / "SKILL.md"
    content = f'''---
name: {name}
description: TODO: Add description for {name}
allowed-tools: ["Read", "Grep", "Glob"]
---

# {name.replace('-', ' ').title()}

TODO: Add skill instructions here.

## When to Use

- TODO: Add trigger scenarios

## How to Use

TODO: Add usage instructions
'''
    skill_dir.mkdir(parents=True, exist_ok=True)
    skill_md.write_text(content)


def fix_add_frontmatter(file_path: Path, frontmatter: dict):
    """Add or replace frontmatter in a markdown file."""
    content = file_path.read_text()

    # Build YAML frontmatter
    fm_lines = ["---"]
    for key, value in frontmatter.items():
        if isinstance(value, list):
            fm_lines.append(f'{key}: {json.dumps(value)}')
        elif isinstance(value, str) and '\n' in value:
            fm_lines.append(f'{key}: |')
            for line in value.split('\n'):
                fm_lines.append(f'  {line}')
        else:
            fm_lines.append(f'{key}: {value}')
    fm_lines.append("---\n")
    fm_str = '\n'.join(fm_lines)

    # Remove existing frontmatter if present
    if content.startswith('---'):
        try:
            end_idx = content.index('---', 3)
            content = content[end_idx + 3:].lstrip('\n')
        except ValueError:
            pass

    file_path.write_text(fm_str + content)


def fix_source_path(marketplace_path: Path, plugin_idx: int, new_source: str):
    """Fix source path in marketplace.json."""
    data = json.loads(marketplace_path.read_text())
    plugins = data.get("plugins", [data])

    if plugin_idx < len(plugins):
        plugins[plugin_idx]["source"] = new_source

    marketplace_path.write_text(json.dumps(data, indent=2) + "\n")


def fix_add_shebang(script_path: Path):
    """Add shebang to Python script."""
    content = script_path.read_text()
    if not content.startswith('#!'):
        content = '#!/usr/bin/env python3\n' + content
        script_path.write_text(content)


def fix_make_executable(script_path: Path):
    """Make script executable."""
    os.chmod(script_path, os.stat(script_path).st_mode | 0o111)


def fix_path_format(marketplace_path: Path, item_type: str, old_path: str, new_path: str):
    """Fix a path format in marketplace.json (e.g., add/remove .md extension)."""
    data = json.loads(marketplace_path.read_text())
    plugins = data.get("plugins", [data])

    for plugin in plugins:
        items = plugin.get(item_type, [])
        for i, item in enumerate(items):
            # Normalize for comparison
            if item.rstrip('/') == old_path.rstrip('/') or item == old_path:
                items[i] = new_path
                break

    marketplace_path.write_text(json.dumps(data, indent=2) + "\n")


# ============================================================================
# VALIDATION FUNCTIONS
# ============================================================================


def extract_path(item: str | dict, path_key: str = "path") -> str:
    """Extract path from string or dictionary format.

    Plugins can define commands/agents/skills in two formats:
    1. String format: "./agents/agent-name.md"
    2. Dictionary format: {"name": "agent-name", "path": "agents/agent-name.md", ...}

    Returns the path string, or empty string if not extractable.
    """
    if isinstance(item, str):
        return item
    elif isinstance(item, dict):
        # Try common path keys
        for key in [path_key, "path", "file", "src"]:
            if key in item:
                return item[key]
        # Fallback: if has "name" but no path, construct default path
        if "name" in item:
            return item["name"]
    return ""


def validate_registration(plugin_root: Path, plugin_data: dict, marketplace_path: Path) -> ValidationResult:
    """Validate marketplace.json entries match actual files."""
    result = ValidationResult()

    registered_commands = plugin_data.get("commands", [])
    registered_agents = plugin_data.get("agents", [])
    registered_skills = plugin_data.get("skills", [])

    # Validate commands
    commands_dir = plugin_root / "commands"
    if commands_dir.exists():
        actual_commands = {f.stem for f in commands_dir.glob("*.md")}
        registered_set = set()

        for cmd_entry in registered_commands:
            # Extract path from string or dictionary format
            cmd = extract_path(cmd_entry)
            if not cmd:
                result.add_warning(f"Command entry has no extractable path: {cmd_entry}")
                continue

            # CRITICAL: Validate path format - commands MUST end with .md
            if not cmd.endswith(".md"):
                correct_path = cmd + ".md"
                result.add_error(
                    f'Command path "{cmd}" missing .md extension (plugin system will fail to load)',
                    Fix(f'Fix path to "{correct_path}"', fix_path_format, marketplace_path, "commands", cmd, correct_path)
                )
                # Still check if the file would exist with correct extension
                name = cmd.replace("./commands/", "").replace("commands/", "")
            else:
                name = cmd.replace("./commands/", "").replace("commands/", "").replace(".md", "")

            registered_set.add(name)

            cmd_file = commands_dir / f"{name}.md"
            if cmd_file.exists():
                if cmd.endswith(".md"):
                    result.add_pass(f"commands/{name}.md registered and exists")
            else:
                result.add_error(
                    f"commands/{name}.md NOT FOUND (registered in marketplace.json)",
                    Fix(f"Create stub commands/{name}.md", fix_create_command_stub, cmd_file, name)
                )

        for actual in actual_commands:
            if actual not in registered_set:
                result.add_error(
                    f"commands/{actual}.md exists but NOT REGISTERED in marketplace.json",
                    Fix(f"Add commands/{actual}.md to marketplace.json",
                        fix_add_to_marketplace, marketplace_path, "commands", f"commands/{actual}.md")
                )

    # Validate agents
    agents_dir = plugin_root / "agents"
    if agents_dir.exists():
        actual_agents = {f.stem for f in agents_dir.glob("*.md")}
        registered_set = set()

        for agent_entry in registered_agents:
            # Extract path from string or dictionary format
            agent = extract_path(agent_entry)
            if not agent:
                result.add_warning(f"Agent entry has no extractable path: {agent_entry}")
                continue

            # CRITICAL: Validate path format - agents MUST end with .md
            if not agent.endswith(".md"):
                correct_path = agent + ".md"
                result.add_error(
                    f'Agent path "{agent}" missing .md extension (plugin system will fail to load)',
                    Fix(f'Fix path to "{correct_path}"', fix_path_format, marketplace_path, "agents", agent, correct_path)
                )
                name = agent.replace("./agents/", "").replace("agents/", "")
            else:
                name = agent.replace("./agents/", "").replace("agents/", "").replace(".md", "")

            registered_set.add(name)

            agent_file = agents_dir / f"{name}.md"
            if agent_file.exists():
                if agent.endswith(".md"):
                    result.add_pass(f"agents/{name}.md registered and exists")
            else:
                result.add_error(
                    f"agents/{name}.md NOT FOUND",
                    Fix(f"Create stub agents/{name}.md", fix_create_agent_stub, agent_file, name)
                )

        for actual in actual_agents:
            if actual not in registered_set:
                result.add_error(
                    f"agents/{actual}.md exists but NOT REGISTERED",
                    Fix(f"Add agents/{actual}.md to marketplace.json",
                        fix_add_to_marketplace, marketplace_path, "agents", f"agents/{actual}.md")
                )

    # Validate skills
    skills_dir = plugin_root / "skills"
    if skills_dir.exists():
        actual_skills = {d.name for d in skills_dir.iterdir() if d.is_dir()}
        registered_set = set()

        for skill_entry in registered_skills:
            # Extract path from string or dictionary format
            skill = extract_path(skill_entry)
            if not skill:
                result.add_warning(f"Skill entry has no extractable path: {skill_entry}")
                continue

            # CRITICAL: Validate path format - skills are directories, must NOT end with .md
            if skill.endswith(".md"):
                correct_path = skill.replace(".md", "").rstrip("/")
                result.add_error(
                    f'Skill path "{skill}" has .md extension but skills are directories',
                    Fix(f'Fix path to "{correct_path}"', fix_path_format, marketplace_path, "skills", skill, correct_path)
                )
                name = skill.replace("./skills/", "").replace("skills/", "").replace(".md", "").rstrip("/")
            else:
                name = skill.replace("./skills/", "").replace("skills/", "").rstrip("/")

            registered_set.add(name)

            skill_md = skills_dir / name / "SKILL.md"
            if skill_md.exists():
                if not skill.endswith(".md"):
                    result.add_pass(f"skills/{name}/SKILL.md registered and exists")
            elif (skills_dir / name).exists():
                result.add_error(
                    f"skills/{name}/ exists but missing SKILL.md",
                    Fix(f"Create skills/{name}/SKILL.md", fix_create_skill_stub, skills_dir / name, name)
                )
            else:
                result.add_error(
                    f"skills/{name}/ NOT FOUND",
                    Fix(f"Create skills/{name}/ with SKILL.md", fix_create_skill_stub, skills_dir / name, name)
                )

        for actual in actual_skills:
            if actual not in registered_set:
                skill_md = skills_dir / actual / "SKILL.md"
                if skill_md.exists():
                    result.add_error(
                        f"skills/{actual}/ exists with SKILL.md but NOT REGISTERED",
                        Fix(f"Add skills/{actual} to marketplace.json",
                            fix_add_to_marketplace, marketplace_path, "skills", f"skills/{actual}")
                    )

    return result


def validate_frontmatter_fields(plugin_root: Path) -> ValidationResult:
    """Validate frontmatter in all markdown files."""
    result = ValidationResult()

    # Commands
    commands_dir = plugin_root / "commands"
    if commands_dir.exists():
        for cmd_file in commands_dir.glob("*.md"):
            content = cmd_file.read_text()
            fm, error = parse_frontmatter(content)

            if error or not fm:
                default_fm = {
                    "description": f"TODO: Add description for {cmd_file.stem}",
                    "argument-hint": "[optional args]",
                    "allowed-tools": ["Read", "Write", "Bash", "Grep", "Glob"]
                }
                result.add_error(
                    f"{cmd_file.name}: {error or 'Missing frontmatter'}",
                    Fix(f"Add frontmatter to {cmd_file.name}", fix_add_frontmatter, cmd_file, default_fm)
                )
                continue

            if not fm.get("description"):
                fm["description"] = f"TODO: Add description for {cmd_file.stem}"
                result.add_error(
                    f"{cmd_file.name}: Missing 'description' field",
                    Fix(f"Add description to {cmd_file.name}", fix_add_frontmatter, cmd_file, fm)
                )
            elif "TODO" in str(fm.get("description", "")):
                result.add_warning(f"{cmd_file.name}: description contains TODO")
            else:
                result.add_pass(f"{cmd_file.name}: frontmatter valid")

    # Agents
    agents_dir = plugin_root / "agents"
    if agents_dir.exists():
        for agent_file in agents_dir.glob("*.md"):
            content = agent_file.read_text()
            fm, error = parse_frontmatter(content)

            if error or not fm:
                default_fm = {
                    "name": agent_file.stem,
                    "description": f"TODO: Add description for {agent_file.stem}",
                    "tools": ["Read", "Write", "Bash", "Grep", "Glob"],
                    "model": "sonnet"
                }
                result.add_error(
                    f"{agent_file.name}: {error or 'Missing frontmatter'}",
                    Fix(f"Add frontmatter to {agent_file.name}", fix_add_frontmatter, agent_file, default_fm)
                )
                continue

            needs_fix = False
            if not fm.get("name"):
                fm["name"] = agent_file.stem
                needs_fix = True
                result.add_error(f"{agent_file.name}: Missing 'name' field")
            if not fm.get("description"):
                fm["description"] = f"TODO: Add description for {agent_file.stem}"
                needs_fix = True
                result.add_error(f"{agent_file.name}: Missing 'description' field")
            if "tools" not in fm:
                # W030: Decision-first approach for missing tools field
                w030_msg = [
                    f"W030: {agent_file.name}: Missing 'tools' field.",
                    "",
                    "üîç DECISION REQUIRED - Ïù¥Í≤ÉÏù¥ ÏùòÎèÑÏ†ÅÏù∏ÏßÄ ÌåêÎã®ÌïòÏÑ∏Ïöî:",
                    "",
                    "  üìã ÌåêÎã® ÌõÑ Ï°∞Ïπò:",
                    "  ‚îú‚îÄ YES (ÏùòÎèÑÏ†Å, Î™®Îì† ÎèÑÍµ¨ ÏÇ¨Ïö©) ‚Üí Î™ÖÏãúÏ†ÅÏúºÎ°ú ÏÑ†Ïñ∏",
                    "  ‚îÇ   tools: [\"*\"]  # ÎòêÎäî tools ÏÉùÎûµ (all tools)",
                    "  ‚îÇ   Ï£ºÏÑù: # Intentionally omitted for full access",
                    "  ‚îÇ",
                    "  ‚îî‚îÄ NO (Ïã§Ïàò, Ï†úÌïú ÌïÑÏöî) ‚Üí ÌïÑÏöîÌïú ÎèÑÍµ¨Îßå Î™ÖÏãú",
                    "      tools: [\"Read\", \"Grep\", \"Glob\"]",
                    "      tools: []  # MCP ÎèÑÍµ¨ ÏóÜÏùå",
                    "",
                    "‚õî tools ÌïÑÎìú ÎàÑÎùΩÏùÑ Î¨¥ÏãúÌïòÏßÄ ÎßàÏÑ∏Ïöî - Î≥¥ÏïàÏóê ÏòÅÌñ•ÏùÑ Ï§Ñ Ïàò ÏûàÏäµÎãàÎã§.",
                    get_skill_hint("W030", "agent tools")
                ]
                result.add_warning("\n".join(w030_msg))

            if needs_fix:
                result.fixes.append(Fix(f"Fix frontmatter in {agent_file.name}", fix_add_frontmatter, agent_file, fm))
            elif fm.get("description") and "TODO" not in str(fm.get("description", "")):
                result.add_pass(f"{agent_file.name}: frontmatter valid")

    # Skills
    skills_dir = plugin_root / "skills"
    if skills_dir.exists():
        for skill_dir in skills_dir.iterdir():
            if not skill_dir.is_dir():
                continue

            skill_md = skill_dir / "SKILL.md"
            if not skill_md.exists():
                continue

            content = skill_md.read_text()
            fm, error = parse_frontmatter(content)

            if error or not fm:
                default_fm = {
                    "name": skill_dir.name,
                    "description": f"TODO: Add description for {skill_dir.name}",
                    "allowed-tools": ["Read", "Grep", "Glob"]
                }
                result.add_error(
                    f"skills/{skill_dir.name}/SKILL.md: {error or 'Missing frontmatter'}",
                    Fix(f"Add frontmatter to skills/{skill_dir.name}/SKILL.md", fix_add_frontmatter, skill_md, default_fm)
                )
                continue

            needs_fix = False
            if not fm.get("name"):
                fm["name"] = skill_dir.name
                needs_fix = True
                result.add_error(f"skills/{skill_dir.name}/SKILL.md: Missing 'name' field")
            if not fm.get("description"):
                fm["description"] = f"TODO: Add description for {skill_dir.name}"
                needs_fix = True
                result.add_error(f"skills/{skill_dir.name}/SKILL.md: Missing 'description' field")
            elif "TODO" in str(fm.get("description", "")):
                result.add_warning(f"skills/{skill_dir.name}/SKILL.md: description contains TODO")
            else:
                if not needs_fix:
                    result.add_pass(f"skills/{skill_dir.name}/SKILL.md: frontmatter valid")

            if needs_fix:
                result.fixes.append(Fix(f"Fix frontmatter in skills/{skill_dir.name}/SKILL.md",
                                       fix_add_frontmatter, skill_md, fm))

    return result


def validate_source_path(plugin_data: dict, marketplace_path: Path, plugin_idx: int) -> ValidationResult:
    """Validate source path format."""
    result = ValidationResult()
    source = plugin_data.get("source", "")

    if isinstance(source, str):
        if source and source not in [".", "./"] and not source.startswith("./"):
            fixed_source = f"./{source}"
            result.add_error(
                f'source "{source}" must start with "./" (e.g., "{fixed_source}")',
                Fix(f'Fix source path to "{fixed_source}"', fix_source_path, marketplace_path, plugin_idx, fixed_source)
            )
        else:
            result.add_pass("source path format valid")
    elif isinstance(source, dict):
        if "source" not in source or "repo" not in source:
            result.add_error('GitHub source format requires {"source": "github", "repo": "user/repo"}')
        else:
            result.add_pass("GitHub source format valid")

    return result


def validate_scripts(plugin_root: Path) -> ValidationResult:
    """Validate script files have shebang and are executable."""
    result = ValidationResult()

    scripts_dir = plugin_root / "scripts"
    if not scripts_dir.exists():
        return result

    for script in scripts_dir.glob("*.py"):
        content = script.read_text()

        # Check shebang
        if not content.startswith('#!'):
            result.add_warning(
                f"scripts/{script.name}: Missing shebang",
                Fix(f"Add shebang to {script.name}", fix_add_shebang, script)
            )
        else:
            result.add_pass(f"scripts/{script.name}: has shebang")

        # Check executable (Unix only)
        if os.name != 'nt':
            if not os.access(script, os.X_OK):
                result.add_warning(
                    f"scripts/{script.name}: Not executable",
                    Fix(f"Make {script.name} executable", fix_make_executable, script)
                )

    return result


def _analyze_keyword_context(content: str, keyword: str, pattern: str) -> List[Dict[str, Any]]:
    """
    Analyze the context around each keyword match to detect false positives.

    Returns list of matches with context analysis:
    - match: the matched text
    - context: surrounding text (¬±30 chars)
    - likely_false_positive: bool
    - reason: why it might be false positive
    """
    import re
    results = []

    # Find all matches with their positions
    for m in re.finditer(pattern, content, re.IGNORECASE):
        start = max(0, m.start() - 30)
        end = min(len(content), m.end() + 30)
        context = content[start:end].replace('\n', ' ')

        likely_fp = False
        reason = ""

        # Check for template variable pattern: {keyword_something}
        template_check = content[max(0, m.start()-1):m.end()+20]
        if re.search(r'\{[^}]*' + keyword + r'[^}]*\}', template_check, re.IGNORECASE):
            likely_fp = True
            reason = "ÌÖúÌîåÎ¶ø Î≥ÄÏàò (e.g., {critical_analysis})"

        # Check for table header pattern: | Keyword |
        table_check = content[max(0, m.start()-3):m.end()+3]
        if re.search(r'\|\s*' + keyword + r'\s*\|', table_check, re.IGNORECASE):
            likely_fp = True
            reason = "ÌÖåÏù¥Î∏î Ìó§Îçî"

        # Check if inside code block (``` ... ```)
        before_content = content[:m.start()]
        code_opens = before_content.count('```')
        if code_opens % 2 == 1:  # Odd number means we're inside a code block
            likely_fp = True
            reason = "ÏΩîÎìú Î∏îÎ°ù ÎÇ¥"

        # Check for inline code (`keyword`)
        inline_check = content[max(0, m.start()-1):m.end()+1]
        if re.search(r'`[^`]*' + keyword, inline_check, re.IGNORECASE):
            likely_fp = True
            reason = "Ïù∏ÎùºÏù∏ ÏΩîÎìú"

        results.append({
            "match": m.group(),
            "context": context,
            "likely_false_positive": likely_fp,
            "reason": reason
        })

    return results


def validate_hookify_compliance(plugin_root: Path) -> ValidationResult:
    """
    W028: Check if MUST/CRITICAL/REQUIRED keywords exist without corresponding hooks.
    W035: Check for 'NOT YET HOOKIFIED' markers indicating known unhookified items.

    Per skillmaker's own principle: "Î¨∏ÏÑú Í∏∞Î∞ò Í∞ïÏ†úÎäî Î¨¥ÏùòÎØ∏Ìï©ÎãàÎã§"

    Enhanced with context-aware analysis to reduce false positives and guide
    proper decision-making (not bypass attempts).
    """
    result = ValidationResult()

    # Enforcement keywords that should be hookified
    enforcement_keywords = [
        (r'\bMUST\b', 'MUST'),
        (r'\bCRITICAL\b', 'CRITICAL'),
        (r'\bREQUIRED\b', 'REQUIRED'),
        (r'\bMANDATORY\b', 'MANDATORY'),
        (r'\bÍ∞ïÏ†ú\b', 'Í∞ïÏ†ú'),
        (r'\bÎ∞òÎìúÏãú\b', 'Î∞òÎìúÏãú')
    ]

    # Unhookified markers
    unhookified_markers = [
        'NOT YET HOOKIFIED',
        'NOT HOOKIFIED',
        '‚ö†Ô∏è **NOT YET HOOKIFIED**'
    ]

    import re

    # Check if hooks.json exists
    hooks_json = plugin_root / "hooks" / "hooks.json"
    has_hooks = hooks_json.exists()

    # Collect files to check
    files_to_check = []

    # Skills
    skills_dir = plugin_root / "skills"
    if skills_dir.exists():
        for skill_dir in skills_dir.iterdir():
            if skill_dir.is_dir():
                skill_md = skill_dir / "SKILL.md"
                if skill_md.exists():
                    files_to_check.append(skill_md)

    # Agents
    agents_dir = plugin_root / "agents"
    if agents_dir.exists():
        for agent_file in agents_dir.glob("*.md"):
            files_to_check.append(agent_file)

    # Commands
    commands_dir = plugin_root / "commands"
    if commands_dir.exists():
        for cmd_file in commands_dir.glob("*.md"):
            files_to_check.append(cmd_file)

    # Track findings with context analysis
    files_with_enforcement = []  # [(rel_path, [analysis_results])]
    unhookified_found = []

    for file_path in files_to_check:
        try:
            content = file_path.read_text()
        except Exception:
            continue

        rel_path = file_path.relative_to(plugin_root)
        file_matches = []

        # Check for enforcement keywords with context analysis
        for pattern, keyword in enforcement_keywords:
            if re.search(pattern, content):
                analysis = _analyze_keyword_context(content, keyword, pattern)
                file_matches.extend(analysis)

        if file_matches:
            files_with_enforcement.append((str(rel_path), file_matches))

        # Check for unhookified markers (W035)
        for marker in unhookified_markers:
            if marker in content:
                # Count occurrences
                count = content.count(marker)
                unhookified_found.append((str(rel_path), count))
                break

    # W028: Enforcement keywords without hooks - with decision guidance
    if files_with_enforcement and not has_hooks:
        # Categorize matches
        likely_rules = []
        likely_fps = []

        for rel_path, matches in files_with_enforcement:
            for m in matches:
                if m["likely_false_positive"]:
                    likely_fps.append((rel_path, m))
                else:
                    likely_rules.append((rel_path, m))

        # Build decision-focused message
        msg_parts = [
            f"W028: {len(files_with_enforcement)} file(s) contain enforcement keywords.",
            "",
            "üîç DECISION REQUIRED - Ïö∞ÌöåÌïòÏßÄ ÎßêÍ≥† Î®ºÏ†Ä ÌåêÎã®ÌïòÏÑ∏Ïöî:",
            ""
        ]

        # Show analysis per file
        for rel_path, matches in files_with_enforcement[:3]:  # Limit to 3 files
            msg_parts.append(f"  üìÑ {rel_path}:")
            for m in matches[:2]:  # Limit to 2 matches per file
                if m["likely_false_positive"]:
                    msg_parts.append(f"     \"{m['match']}\" ‚Üí ‚ö†Ô∏è {m['reason']} (false positive Í∞ÄÎä•)")
                else:
                    msg_parts.append(f"     \"{m['match']}\" ‚Üí üî¥ Í∑úÏπôÏúºÎ°ú Î≥¥ÏûÑ (hook ÌïÑÏöî Í∞ÄÎä•)")

        msg_parts.extend([
            "",
            "üìã ÌåêÎã® ÌõÑ Ï°∞Ïπò:",
            "  ‚îú‚îÄ YES (ÏßÑÏßú Í∑úÏπô) ‚Üí hookÏúºÎ°ú Í∞ïÏ†ú ÌïÑÏöî",
            "  ‚îÇ   Í≤ΩÎ°ú: /skillmaker:hook-templates ÎòêÎäî /hookify",
            "  ‚îÇ   Ï∞∏Ï°∞: Skill(\"skillmaker:hook-sdk-integration\")",
            "  ‚îÇ",
            "  ‚îî‚îÄ NO (false positive) ‚Üí Ï†ïÎãπÌïú Ïö©Ïñ¥ Î≥ÄÍ≤Ω",
            "      - ÌÖåÏù¥Î∏î Ìó§Îçî: Required ‚Üí ÌïÑÏàò",
            "      - ÌÖúÌîåÎ¶ø Î≥ÄÏàò: {critical_X} ‚Üí {critique_X}",
            "      - ÎòêÎäî hooks/hooks.json Îπà ÌååÏùº ÏÉùÏÑ± (Í∑úÏπô ÏóÜÏùåÏùÑ Î™ÖÏãú)",
            "",
            "‚õî ÌÇ§ÏõåÎìúÎßå Î∞îÍøîÏÑú Í≤ΩÍ≥†Î•º Ïö∞ÌöåÌïòÎäî Í≤ÉÏùÄ Í∏àÏßÄÎê©ÎãàÎã§."
        ])

        result.add_warning("\n".join(msg_parts))
    elif files_with_enforcement and has_hooks:
        # hooks.json exists, that's good
        result.add_pass(f"W028: Enforcement keywords found in {len(files_with_enforcement)} files, hooks.json exists")

    # W035: Unhookified markers found
    for rel_path, count in unhookified_found:
        hint = get_skill_hint("W035")
        result.add_warning(
            f"W035: {rel_path}: Contains {count} 'NOT YET HOOKIFIED' marker(s) - "
            f"known limitation awaiting hookification{hint}"
        )

    if not unhookified_found and files_to_check:
        result.add_pass("W035: No unhookified markers found")

    return result


def validate_unnecessary_files(plugin_root: Path) -> ValidationResult:
    """
    W036: Detect unnecessary files that should be cleaned up or gitignored.

    Categories:
    - DELETE: Files that should be removed (logs, caches)
    - GITIGNORE: Files that should be in .gitignore
    - SENSITIVE: Files that may contain secrets (.env)
    """
    result = ValidationResult()

    # Patterns to detect with recommendations
    # Format: (pattern, category, description, recommendation)
    patterns = [
        # Log files - DELETE
        ("firebase-debug.log", "DELETE", "Firebase debug log", "rm firebase-debug.log"),
        ("npm-debug.log", "DELETE", "NPM debug log", "rm npm-debug.log"),
        ("yarn-error.log", "DELETE", "Yarn error log", "rm yarn-error.log"),
        ("debug.log", "DELETE", "Debug log", "rm debug.log"),
        ("*.log", "DELETE", "Log files", "rm *.log"),

        # Cache directories - DELETE
        ("__pycache__", "DELETE", "Python cache", "rm -rf __pycache__"),
        (".pytest_cache", "DELETE", "Pytest cache", "rm -rf .pytest_cache"),
        (".mypy_cache", "DELETE", "Mypy cache", "rm -rf .mypy_cache"),
        ("node_modules", "DELETE", "Node modules (large)", "rm -rf node_modules"),
        (".cache", "DELETE", "Cache directory", "rm -rf .cache"),

        # System files - GITIGNORE
        (".DS_Store", "GITIGNORE", "macOS metadata", 'echo ".DS_Store" >> .gitignore'),
        ("Thumbs.db", "GITIGNORE", "Windows thumbnail", 'echo "Thumbs.db" >> .gitignore'),

        # IDE files - GITIGNORE (optional, some prefer to keep)
        (".idea", "GITIGNORE", "JetBrains IDE config", 'echo ".idea/" >> .gitignore'),
        (".vscode", "GITIGNORE", "VS Code config", 'echo ".vscode/" >> .gitignore'),

        # Sensitive files - SENSITIVE (should never be committed)
        (".env", "SENSITIVE", "Environment variables", "‚ö†Ô∏è Contains secrets - do not commit"),
        (".env.local", "SENSITIVE", "Local environment", "‚ö†Ô∏è Contains secrets - do not commit"),
        (".env.production", "SENSITIVE", "Production secrets", "‚ö†Ô∏è Contains secrets - do not commit"),
        ("credentials.json", "SENSITIVE", "Credentials file", "‚ö†Ô∏è Contains secrets - do not commit"),
        ("*.pem", "SENSITIVE", "Private key", "‚ö†Ô∏è Contains secrets - do not commit"),
        ("*.key", "SENSITIVE", "Private key", "‚ö†Ô∏è Contains secrets - do not commit"),
    ]

    found_issues = {
        "DELETE": [],
        "GITIGNORE": [],
        "SENSITIVE": []
    }

    # Check for files matching patterns
    for pattern, category, description, recommendation in patterns:
        if "*" in pattern:
            # Glob pattern
            matches = list(plugin_root.glob(pattern))
            # Also check in subdirectories (one level)
            for subdir in plugin_root.iterdir():
                if subdir.is_dir() and not subdir.name.startswith("."):
                    matches.extend(subdir.glob(pattern))
        else:
            # Exact match
            target = plugin_root / pattern
            matches = [target] if target.exists() else []

        for match in matches:
            rel_path = match.relative_to(plugin_root)
            found_issues[category].append((str(rel_path), description, recommendation))

    # Check .gitignore for proper entries
    gitignore_path = plugin_root / ".gitignore"
    gitignore_content = gitignore_path.read_text() if gitignore_path.exists() else ""

    # Generate warnings
    if found_issues["SENSITIVE"]:
        msg_parts = [
            "W036: SENSITIVE files detected - potential security risk!",
            "",
            "üî¥ These files may contain secrets and should NEVER be committed:",
            ""
        ]
        for path, desc, rec in found_issues["SENSITIVE"]:
            msg_parts.append(f"  ‚Ä¢ {path} ({desc})")
            msg_parts.append(f"    {rec}")
        msg_parts.extend([
            "",
            "üìã Recommended actions:",
            "  1. Add to .gitignore IMMEDIATELY",
            "  2. If already committed, use 'git rm --cached <file>'",
            "  3. Consider rotating any exposed secrets"
        ])
        result.add_warning("\n".join(msg_parts))

    if found_issues["DELETE"]:
        msg_parts = [
            "W036: Unnecessary files found - cleanup recommended:",
            ""
        ]
        for path, desc, rec in found_issues["DELETE"][:10]:  # Limit to 10
            msg_parts.append(f"  ‚Ä¢ {path} ({desc})")
        if len(found_issues["DELETE"]) > 10:
            msg_parts.append(f"  ... and {len(found_issues['DELETE']) - 10} more")
        msg_parts.extend([
            "",
            "üìã Cleanup commands:",
        ])
        # Group by recommendation
        seen_recs = set()
        for _, _, rec in found_issues["DELETE"]:
            if rec not in seen_recs:
                msg_parts.append(f"  {rec}")
                seen_recs.add(rec)
        result.add_warning("\n".join(msg_parts))

    if found_issues["GITIGNORE"]:
        # Check if already in .gitignore
        not_ignored = []
        for path, desc, rec in found_issues["GITIGNORE"]:
            base_name = Path(path).name
            if base_name not in gitignore_content:
                not_ignored.append((path, desc, rec))

        if not_ignored:
            msg_parts = [
                "W036: Files that should be in .gitignore:",
                ""
            ]
            for path, desc, rec in not_ignored:
                msg_parts.append(f"  ‚Ä¢ {path} ({desc})")
            msg_parts.extend([
                "",
                "üìã Add to .gitignore:",
            ])
            for path, _, _ in not_ignored:
                base = Path(path).name
                if base.startswith("."):
                    msg_parts.append(f'  echo "{base}" >> .gitignore')
                else:
                    msg_parts.append(f'  echo "{base}/" >> .gitignore')
            result.add_warning("\n".join(msg_parts))

    if not any(found_issues.values()):
        result.add_pass("W036: No unnecessary files detected")

    return result


def validate_settings_json() -> ValidationResult:
    """Check for common settings.json misconfigurations."""
    result = ValidationResult()
    home = Path.home()

    settings_paths = [
        home / ".claude" / "settings.json",
        home / ".config" / "claude-code" / "settings.json",
    ]

    for settings_path in settings_paths:
        if not settings_path.exists():
            continue

        try:
            settings = json.loads(settings_path.read_text())
        except (json.JSONDecodeError, IOError):
            continue

        plugins = settings.get("plugins", [])
        for i, plugin in enumerate(plugins):
            if isinstance(plugin, dict):
                source = plugin.get("source", "")
                if isinstance(source, str) and source:
                    if not source.startswith("./") and not source.startswith("/"):
                        result.add_error(
                            f'settings.json plugins[{i}].source "{source}" must start with "./"'
                        )

        marketplaces = settings.get("extraKnownMarketplaces", {})
        for name, config in marketplaces.items():
            source = config.get("source", "")
            if isinstance(source, str) and source and not source.startswith("./"):
                result.add_error(
                    f'settings.json extraKnownMarketplaces.{name}.source must start with "./"'
                )

    return result


def apply_fixes(fixes: List[Fix], dry_run: bool = False) -> Tuple[int, int]:
    """Apply all fixes. Returns (success_count, fail_count)."""
    success = 0
    fail = 0

    print("\n" + "=" * 60)
    print("APPLYING FIXES" if not dry_run else "FIXES PREVIEW (dry-run)")
    print("=" * 60)

    for fix in fixes:
        print(f"\n{'[DRY-RUN] ' if dry_run else ''}‚Üí {fix.description}")

        if dry_run:
            success += 1
        else:
            if fix.apply():
                print(f"  ‚úÖ Done")
                success += 1
            else:
                fail += 1

    return success, fail


def main():
    # Parse arguments
    json_output = "--json" in sys.argv
    fix_mode = "--fix" in sys.argv
    dry_run = "--dry-run" in sys.argv
    args = [a for a in sys.argv[1:] if not a.startswith("--")]

    plugin_root = Path(args[0]).resolve() if args else Path.cwd()

    # Find marketplace.json
    marketplace_path, legacy_warning = find_marketplace_json(plugin_root)
    if not marketplace_path:
        if json_output:
            print(json.dumps({"status": "error", "message": "No marketplace.json found"}))
        else:
            print("ERROR: No .claude-plugin/marketplace.json found")
            print("       Claude Code requires: .claude-plugin/marketplace.json")
        sys.exit(1)

    # Show legacy format warning (plugin.json found but not supported)
    if legacy_warning:
        if json_output:
            print(json.dumps({"status": "error", "message": legacy_warning}))
            sys.exit(1)
        else:
            print(f"\n{legacy_warning}\n")
            print("=" * 60)
            print("VALIDATION BLOCKED - Fix legacy format before proceeding")
            print("=" * 60)
            sys.exit(1)

    # Check for marketplace.json and plugin.json conflict
    # This causes "no context" error in Claude Code runtime
    claude_plugin_dir = plugin_root / ".claude-plugin"
    if claude_plugin_dir.exists():
        has_marketplace = (claude_plugin_dir / "marketplace.json").exists()
        has_plugin_json = (claude_plugin_dir / "plugin.json").exists()
        if has_marketplace and has_plugin_json:
            error_msg = (
                "CONFLICT: .claude-plugin/ contains both marketplace.json and plugin.json\n"
                "  This causes 'no context' error during plugin installation.\n"
                "  Solution: Remove plugin.json when using marketplace.json for distribution."
            )
            if json_output:
                print(json.dumps({"status": "error", "message": error_msg}))
            else:
                print(f"ERROR: {error_msg}")
            sys.exit(1)

    # Parse marketplace.json
    try:
        data = json.loads(marketplace_path.read_text())
    except json.JSONDecodeError as e:
        if json_output:
            print(json.dumps({"status": "error", "message": f"Invalid JSON: {e}"}))
        else:
            print(f"ERROR: Invalid JSON in {marketplace_path}: {e}")
        sys.exit(1)

    # Get plugins
    plugins = data.get("plugins", [data])

    # Run all validations
    total_result = ValidationResult()

    # Settings.json validation
    total_result.merge(validate_settings_json())

    for i, plugin in enumerate(plugins):
        source = plugin.get("source", "./")
        if source in [".", "./"]:
            effective_root = plugin_root
        else:
            effective_root = plugin_root / source.lstrip("./")

        total_result.merge(validate_source_path(plugin, marketplace_path, i))
        total_result.merge(validate_registration(effective_root, plugin, marketplace_path))
        total_result.merge(validate_frontmatter_fields(effective_root))
        total_result.merge(validate_scripts(effective_root))
        total_result.merge(validate_hookify_compliance(effective_root))
        total_result.merge(validate_unnecessary_files(effective_root))

    # Output results
    if json_output:
        output = {
            "status": "fail" if total_result.errors else ("warn" if total_result.warnings else "pass"),
            "errors": total_result.errors,
            "warnings": total_result.warnings,
            "passed": len(total_result.passed),
            "fixable": len(total_result.fixes)
        }
        print(json.dumps(output, indent=2))
    else:
        print("=" * 60)
        print("PLUGIN VALIDATION")
        print("=" * 60)
        print(f"Plugin: {plugin_root}")
        print()

        if total_result.errors:
            print("ERRORS:")
            for e in total_result.errors:
                print(f"  ‚ùå {e}")
            print()

        if total_result.warnings:
            print("WARNINGS:")
            for w in total_result.warnings:
                print(f"  ‚ö†Ô∏è  {w}")
            print()

        print("SUMMARY:")
        print(f"  Errors:   {len(total_result.errors)}")
        print(f"  Warnings: {len(total_result.warnings)}")
        print(f"  Passed:   {len(total_result.passed)}")
        print(f"  Fixable:  {len(total_result.fixes)}")
        print()

        if total_result.errors:
            print("STATUS: ‚ùå DEPLOYMENT WILL FAIL")
        elif total_result.warnings:
            print("STATUS: ‚ö†Ô∏è  DEPLOYMENT MAY HAVE ISSUES")
        else:
            print("STATUS: ‚úÖ READY FOR DEPLOYMENT")

    # Apply fixes if requested
    if fix_mode and total_result.fixes:
        success, fail = apply_fixes(total_result.fixes, dry_run)
        print(f"\nFixes applied: {success}, Failed: {fail}")

        if not dry_run and success > 0:
            print("\nüí° Re-run validation to verify fixes:")
            print(f"   python3 {sys.argv[0]}")
    elif fix_mode and not total_result.fixes:
        print("\n‚ú® Nothing to fix!")
    elif total_result.fixes and not json_output:
        print(f"\nüí° {len(total_result.fixes)} issues can be auto-fixed. Run with --fix:")
        print(f"   python3 {sys.argv[0]} --fix")
        print(f"   python3 {sys.argv[0]} --fix --dry-run  # Preview only")

    # Exit code
    if total_result.errors and not (fix_mode and not dry_run):
        sys.exit(1)
    elif total_result.warnings:
        sys.exit(2)
    else:
        sys.exit(0)


if __name__ == "__main__":
    main()
