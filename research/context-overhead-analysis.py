#!/usr/bin/env python3
"""
Context Overhead Analysis - Precise Token Measurement

Measures the exact token overhead of MCP loading in Claude Code sessions.
"""

import subprocess
import json
import time
from pathlib import Path


def measure_claude_context():
    """Measure context by parsing /context output from Claude Code."""

    results = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "tests": []
    }

    # Test 1: Measure tokens with Serena MCP config
    print("=" * 60)
    print("CONTEXT OVERHEAD MEASUREMENT")
    print("=" * 60)

    # Create minimal MCP config
    mcp_config = {
        "mcpServers": {
            "serena": {
                "command": "uvx",
                "args": [
                    "--from", "git+https://github.com/oraios/serena",
                    "serena", "start-mcp-server", "--project-from-cwd"
                ]
            }
        }
    }

    config_path = Path("/tmp/serena-test.mcp.json")
    with open(config_path, "w") as f:
        json.dump(mcp_config, f)

    # Measure startup time and initial tokens
    prompt = """Return ONLY this JSON, nothing else:
{"status": "ok", "mcp_tools_count": <count of mcp__serena__ tools available>}"""

    print("\nðŸ”¬ Test: Subprocess with Serena MCP")
    print("-" * 40)

    start = time.time()
    cmd = [
        "claude",
        "--mcp-config", str(config_path),
        "-p", prompt,
        "--output-format", "json",
        "--max-turns", "1"
    ]

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=120,
            cwd="/home/chanhokim/projects/personal_project/claude-plugin/skillmaker"
        )

        duration = time.time() - start

        print(f"â±ï¸  Total time: {duration:.1f}s")
        print(f"ðŸ“¤ Exit code: {result.returncode}")

        if result.stdout:
            try:
                output = json.loads(result.stdout)

                # Parse token usage
                if "usage" in output:
                    usage = output["usage"]
                    input_tokens = usage.get("input_tokens", 0)
                    output_tokens = usage.get("output_tokens", 0)

                    print(f"\nðŸ’° Token Usage in Subprocess:")
                    print(f"   Input tokens: {input_tokens}")
                    print(f"   Output tokens: {output_tokens}")

                # Check response content
                if "result" in output:
                    print(f"\nðŸ“‹ Response content:")
                    print(f"   {output['result'][:500]}")

                results["tests"].append({
                    "name": "subprocess_with_mcp",
                    "duration_s": duration,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "success": True
                })

            except json.JSONDecodeError as e:
                print(f"âš ï¸  JSON parse error: {e}")
                print(f"   Raw output: {result.stdout[:300]}")
                results["tests"].append({
                    "name": "subprocess_with_mcp",
                    "duration_s": duration,
                    "success": False,
                    "error": str(e)
                })

    except subprocess.TimeoutExpired:
        print("âŒ Timeout")
        results["tests"].append({
            "name": "subprocess_with_mcp",
            "success": False,
            "error": "timeout"
        })

    except Exception as e:
        print(f"âŒ Error: {e}")
        results["tests"].append({
            "name": "subprocess_with_mcp",
            "success": False,
            "error": str(e)
        })

    # Analysis summary
    print("\n" + "=" * 60)
    print("ANALYSIS")
    print("=" * 60)

    successful_tests = [t for t in results["tests"] if t.get("success")]

    if successful_tests:
        test = successful_tests[0]
        print(f"""
ðŸ“Š Subprocess Isolation Findings:

   1. STARTUP OVERHEAD
      - Time: {test.get('duration_s', 0):.1f}s (MCP server initialization)
      - This is a one-time cost per subprocess

   2. TOKEN CONSUMPTION IN SUBPROCESS
      - Input tokens: {test.get('input_tokens', 'N/A')}
      - Output tokens: {test.get('output_tokens', 'N/A')}
      - Note: This includes MCP tool definitions

   3. MAIN SESSION SAVINGS
      - If MCP is NOT loaded in main session: 0 tokens overhead
      - Typical MCP tool definition: ~200-500 tokens per tool
      - Serena has ~30+ tools = potential 6,000-15,000 tokens saved

   4. TRADE-OFF ANALYSIS
      - Subprocess startup: ~30-60s
      - Token savings: 6,000-15,000 tokens per main session
      - Best for: Infrequent MCP usage, token-constrained scenarios
      - Worst for: Frequent MCP calls, latency-sensitive workflows
""")
    else:
        print("\nâŒ No successful tests to analyze")

    # Save results
    output_path = Path(__file__).parent / "context-overhead-results.json"
    with open(output_path, "w") as f:
        json.dump(results, f, indent=2)

    print(f"\nðŸ’¾ Results saved to: {output_path}")

    return results


def estimate_mcp_token_overhead():
    """Estimate token overhead based on MCP tool count."""
    print("\n" + "=" * 60)
    print("MCP TOKEN OVERHEAD ESTIMATION")
    print("=" * 60)

    # Known values from typical MCP servers
    mcp_overhead_estimates = {
        "serena": {
            "tool_count": 30,
            "avg_tokens_per_tool": 350,
            "total_estimate": 30 * 350
        },
        "context7": {
            "tool_count": 2,
            "avg_tokens_per_tool": 200,
            "total_estimate": 2 * 200
        },
        "playwright": {
            "tool_count": 25,
            "avg_tokens_per_tool": 250,
            "total_estimate": 25 * 250
        },
        "greptile": {
            "tool_count": 12,
            "avg_tokens_per_tool": 300,
            "total_estimate": 12 * 300
        }
    }

    print(f"""
ðŸ“Š Estimated MCP Token Overhead (per session):

   | MCP Server  | Tools | Est. Tokens | Subprocess Benefit |
   |-------------|-------|-------------|-------------------|
   | Serena      | 30    | ~10,500     | âœ… High savings   |
   | Playwright  | 25    | ~6,250      | âœ… High savings   |
   | Greptile    | 12    | ~3,600      | âš ï¸  Medium savings |
   | Context7    | 2     | ~400        | âŒ Not worth it   |

   Rule of thumb:
   - > 5,000 tokens: Subprocess isolation recommended
   - 2,000-5,000 tokens: Consider based on usage frequency
   - < 2,000 tokens: Keep in main session

   Break-even point:
   - If you call MCP < 3 times per session: Use subprocess
   - If you call MCP > 10 times per session: Use agent gateway
""")

    return mcp_overhead_estimates


if __name__ == "__main__":
    results = measure_claude_context()
    estimates = estimate_mcp_token_overhead()
